{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "divine_transformer_big.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "IktUvj6AndiG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972145353,
     "user_tz": -120,
     "elapsed": 523,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    },
    "outputId": "6b8e831f-b52d-45a8-8377-4fd5f48d66b1"
   },
   "source": [
    "import io\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "'''\n",
    "ORIGINALS PARAMETERS FROM TENSORFLOW\n",
    "\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "'''\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "EPOCHS = 20\n",
    "num_layers = 4\n",
    "d_model = 256\n",
    "dff = 1024\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "BATCH_SIZE = 64\n",
    "PRINT_EACH_BATCHES = 100\n",
    "\n",
    "N_BEAM = 2\n",
    "K = 20\n",
    "TEMP = 1.4\n",
    "\n",
    "sliding_window_X = 6\n",
    "sliding_window_y = 3\n",
    "SAVING_STEP = 5\n",
    "\n",
    "##########################\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  3 23:14:41 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.67       Driver Version: 460.67       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX130       Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   44C    P8    N/A /  N/A |   1845MiB /  2004MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       834      G   /usr/lib/Xorg                       2MiB |\n",
      "|    0   N/A  N/A      8029      C   ...ingCuda/venv38/bin/python     1839MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNtKlcYAqGYw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972148266,
     "user_tz": -120,
     "elapsed": 3426,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    },
    "outputId": "a2710b89-5341-457d-8350-df2c2a9b1302"
   },
   "source": [
    "\n",
    "# UNCOMMENT IF YOU ARE ON COLAB\n",
    "\n",
    "!pip install levenshtein\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "root_path = 'gdrive/My Drive/transformer/'\n",
    "\n",
    "\n",
    "# REMEMBER TO SET THE ROOT PATH WITH A REGEX IN ALL THE NOTEBOOK\n",
    "# gdrive/My Drive/transformer/\n",
    "\n",
    "from Levenshtein import distance as levenshtein_distance"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwsdLDJMn0ar"
   },
   "source": [
    "## ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_qm-mmfSnv-y",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972148267,
     "user_tz": -120,
     "elapsed": 3415,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    }
   },
   "source": [
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        # (batch_size, seq_len, d_model)\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        # (batch_size, num_heads, seq_len_*, depth)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention,\n",
    "                                        perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        return output, attention_weights\n",
    "\n"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfdLb_Y0HkIs"
   },
   "source": [
    "# BEAM SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HB-Uhb6XHjUU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972148268,
     "user_tz": -120,
     "elapsed": 3402,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    }
   },
   "source": [
    "class Flow:\n",
    "    def __init__(self, init, prob):\n",
    "        self.output = init\n",
    "        self.prob = prob\n",
    "\n",
    "def sampling_flows(flows_array):\n",
    "    all_flows = copy.deepcopy(flows_array)\n",
    "    drawn_flows = []\n",
    "    for _ in range(N_BEAM):\n",
    "        top_k_prob = [f.prob for f in all_flows]\n",
    "        top_k_prob = np.array([e if e > 0 else 0.05 for e in top_k_prob])\n",
    "        top_k_prob = top_k_prob ** TEMP\n",
    "        prob_sum = sum(top_k_prob)\n",
    "        r = random.random() * prob_sum\n",
    "        j = 0\n",
    "        while True:\n",
    "            if r - top_k_prob[j] <= 0:\n",
    "                drawn_flows.append(all_flows[j])\n",
    "                all_flows.pop(j)\n",
    "                break\n",
    "            r -= top_k_prob[j]\n",
    "            j += 1\n",
    "    return drawn_flows\n",
    "\n",
    "\n",
    "def next_predictions(encoder_input, output, k, argmax):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "    predictions, attention_weights = transformer(encoder_input,\n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n",
    "    if not argmax:\n",
    "        top_k_id_, top_k_prob_ = (tf.nn.top_k(predictions, k=k)[1]).numpy()[0][0], \\\n",
    "                                 (tf.nn.top_k(predictions, k=k)[0]).numpy()[0][0]\n",
    "        return top_k_id_, top_k_prob_\n",
    "    else:\n",
    "        return tf.argmax(predictions, axis=-1)\n",
    "\n",
    "\n",
    "def beam_search(sentence, two_way_X, two_way_y, max_length=1000):\n",
    "    encoder_input = tf.cast(tf.convert_to_tensor([tokenize(two_way_X, sentence, X=True)]), tf.int64)\n",
    "    t_init, t_end = two_way_y.get('<t_init>'), two_way_y.get('<t_end>')\n",
    "    start, end = two_way_y.get('<start>'), two_way_y.get('<end>')\n",
    "    space = tf.cast(tf.convert_to_tensor([two_way_y.get('<s>')]), tf.int64)\n",
    "    syl = tf.cast(tf.convert_to_tensor([two_way_y.get('<syl>')]), tf.int64)\n",
    "    output = tf.convert_to_tensor([t_init])\n",
    "    output = tf.expand_dims(output, 0)\n",
    "    output = tf.cast(output, tf.int64)\n",
    "\n",
    "    flows_array = [Flow(output, 1)]\n",
    "    best_flows = []\n",
    "    for u in range(max_length):\n",
    "        new_flow_array = []\n",
    "        for e in flows_array:\n",
    "            prev_pred = e.output.numpy().flatten()\n",
    "            if (len(prev_pred) > 1 and prev_pred[-2] == space and prev_pred[-1] == syl) or \\\n",
    "                    (len(prev_pred) > 1 and prev_pred[-2] == start and prev_pred[-1] == syl):\n",
    "                top_k_id, top_k_prob = next_predictions(encoder_input, e.output, k=K, argmax=False)\n",
    "                for i in range(len(top_k_id)):\n",
    "                    new_out = tf.concat(\n",
    "                        [(copy.deepcopy(e)).output, tf.cast(tf.convert_to_tensor([[top_k_id[i]]]), tf.int64)], axis=-1)\n",
    "                    new_flow_array.append(Flow(new_out, top_k_prob[i] * e.prob))\n",
    "            else:\n",
    "                new_out = tf.concat(\n",
    "                    [(copy.deepcopy(e)).output, next_predictions(encoder_input, e.output, k=K, argmax=True)], axis=-1)\n",
    "                new_flow_array.append(Flow(new_out, e.prob))\n",
    "\n",
    "        flows_array = new_flow_array\n",
    "        # flows_array = sorted(flows_array, key=lambda f: f.prob, reverse=True)[:N_BEAM]\n",
    "        if len(flows_array) > N_BEAM:\n",
    "            flows_array = sampling_flows(flows_array)\n",
    "\n",
    "        to_pop = []\n",
    "        for j, flow in enumerate(flows_array):\n",
    "            if flow.output.numpy().flatten()[-1] == t_end:\n",
    "                best_flows.append(flow)\n",
    "                to_pop.append(flow)\n",
    "        [flows_array.remove(p) for p in to_pop]\n",
    "        if len(best_flows) == N_BEAM:\n",
    "            result = (max(best_flows, key=lambda f: f.prob)).output\n",
    "            text = detokenize(two_way_y, result)\n",
    "            return text"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkJy3R0DrjLz"
   },
   "source": [
    "# ENCODER DECODER"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rw8o-6mbrdAu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972148598,
     "user_tz": -120,
     "elapsed": 3727,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    }
   },
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        return out2\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "                 maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "            attention_weights[f'decoder_layer{i + 1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i + 1}_block2'] = block2\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights\n"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmSemJjjpNfH"
   },
   "source": [
    "# TRANSFORMER TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QCMw34fepNHC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972148598,
     "user_tz": -120,
     "elapsed": 3721,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    }
   },
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='gelu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuNecFH6o1nU"
   },
   "source": [
    "# TRANSFORMER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7UwyGFL0otlj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972148599,
     "user_tz": -120,
     "elapsed": 3715,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    }
   },
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                 target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.tokenizer = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                                 input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        return final_output, attention_weights"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk3GO3wh8e16"
   },
   "source": [
    "# ENCODE TOKENIZE \\& PERSONAL TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VZGkOv_U8diI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972148816,
     "user_tz": -120,
     "elapsed": 3927,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    }
   },
   "source": [
    "def encode_dataset(X, y):\n",
    "    \"\"\"\n",
    "        Creates integer encoded version of the dataset.\n",
    "    \"\"\"\n",
    "    encoded_X = TwoWay()\n",
    "    encoded_set = []\n",
    "    for row in X:\n",
    "        tmp_row = re.sub(r'<start>|<end>|<syl>', r'', row)\n",
    "        [[encoded_set.append(c) for c in w] for w in tmp_row.split('<s>')]\n",
    "    encoded_set += ['<start>', '<end>', '<s>', '<t_init>', '<t_end>']\n",
    "    encoded_set = set(encoded_set)\n",
    "    [encoded_X.add(i + 1, w) for i, w in enumerate(encoded_set)]\n",
    "    with open('gdrive/My Drive/transformer/resources/encoded_X.pickle', 'wb') as handle:\n",
    "        pickle.dump(encoded_X, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    encoded_y = TwoWay()\n",
    "    encoded_set = []\n",
    "    for row in y:\n",
    "        tmp_row = re.sub(r'<syl>', r'<s>', row)\n",
    "        tmp_row = re.sub(r'<start>|<end>|<c>', r'', tmp_row)\n",
    "        [encoded_set.append(w) for w in tmp_row.split('<s>')]\n",
    "    encoded_set += ['<syl>', '<s>', '<start>', '<end>', '<t_init>', '<t_end>', '<c>']\n",
    "    encoded_set = set(encoded_set)\n",
    "    encoded_set.remove(\"\")\n",
    "    [encoded_y.add(i + 1, w) for i, w in enumerate(encoded_set)]\n",
    "    with open('gdrive/My Drive/transformer/resources/encoded_y.pickle', 'wb') as handle:\n",
    "        pickle.dump(encoded_y, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"Data saved successfully!\")\n",
    "\n",
    "\n",
    "def tokenize(two_way, line, X):\n",
    "    spaced_line = re.sub(r'<', r' <', line)\n",
    "    spaced_line = re.sub(r'>', r'> ', spaced_line)\n",
    "    spaced_line = re.sub(r'^ | $', r'', spaced_line)\n",
    "    spaced_line = re.sub(r'[ ]+', r' ', spaced_line)\n",
    "    spaced_line = spaced_line.split(' ')\n",
    "    while True:\n",
    "        try:\n",
    "            spaced_line.remove('')\n",
    "        except ValueError:\n",
    "            break\n",
    "\n",
    "    if X:\n",
    "        tok_X = []\n",
    "        for w in spaced_line:\n",
    "            if w in ['<start>', '<end>', '<s>', '<t_init>', '<t_end>', '<c>']:\n",
    "                tok_X.append(two_way.get(w))\n",
    "            else:\n",
    "                [tok_X.append(two_way.get(c)) for c in w]\n",
    "        return tok_X\n",
    "    else:\n",
    "        return [two_way.get(e) for e in spaced_line]\n",
    "\n",
    "\n",
    "def detokenize(two_way, line):\n",
    "    sentence = [two_way.get(e.numpy()) for e in line[0]]\n",
    "    return ''.join(sentence)\n",
    "\n",
    "\n",
    "def detokenize_(two_way, line):\n",
    "    sentence = [two_way.get(e) for e in line]\n",
    "    return ''.join(sentence)\n",
    "\n",
    "\n",
    "def make_human_understandable(sentence, keep_syl=True):\n",
    "    sentence = re.sub(r'<start>|<t_init>|<t_end>|<end>', r'', sentence)\n",
    "    if keep_syl:\n",
    "        sentence = re.sub(r'<syl>', r'|', sentence)\n",
    "    else:\n",
    "        sentence = re.sub(r'<syl>', r'', sentence)\n",
    "    sentence = re.sub(r'<s>', r' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def tokenize_pairs(X, y):\n",
    "\n",
    "    X_tok = []\n",
    "    y_tok = []\n",
    "    for i in range(len(X)):\n",
    "        next_to_check = ''\n",
    "        for s in X[i:i + sliding_window_X + sliding_window_y]:\n",
    "            next_to_check += s\n",
    "        if not re.search(r'<canto>', next_to_check):\n",
    "            terz = [tokenize(two_way_X, '<t_init>', X=True)[0]]\n",
    "            terz_y = [tokenize(two_way_y, '<t_init>', X=False)[0]]\n",
    "            for k in range(sliding_window_X):\n",
    "                if (k % 3 == 0) and (k > 0):\n",
    "                    terz += tokenize(two_way_X, '<t_end>', X=True)\n",
    "                    terz += tokenize(two_way_X, '<t_init>',X=True)\n",
    "                terz += (tokenize(two_way_X, X[i + k], X=True))\n",
    "\n",
    "            terz += tokenize(two_way_X, '<t_end>', X=True)\n",
    "            for j in range(sliding_window_y):\n",
    "                terz_y += (tokenize(two_way_y, y[i + j + sliding_window_X], X=False))\n",
    "            terz_y += tokenize(two_way_y, '<t_end>', X=False)\n",
    "            X_tok.append(terz)\n",
    "            y_tok.append(terz_y)\n",
    "\n",
    "    return X_tok, y_tok\n",
    "\n",
    "\n",
    "def make_batches(X_y_tok, batch_size):\n",
    "    batches = []\n",
    "    X_tok, y_tok = X_y_tok\n",
    "    for i in range(0, len(X_tok), batch_size):\n",
    "        if (batch_size + i) < len(X_tok):\n",
    "            batches.append((tf.cast(tf.ragged.constant(X_tok[i:i + batch_size]), tf.int64).to_tensor(),\n",
    "                            (tf.cast(tf.ragged.constant(y_tok[i:i + batch_size]), tf.int64).to_tensor())))\n",
    "        else:\n",
    "            break\n",
    "    return batches"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUWxR546pUSP"
   },
   "source": [
    "# TRANSFORMER TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wsly3FnGuAcW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972148816,
     "user_tz": -120,
     "elapsed": 3922,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    },
    "outputId": "298aa97a-722c-4396-a0f1-e13edca888d7"
   },
   "source": [
    "!ls"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divine_transformer.ipynb  README.md  transformer_data\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nzS2YHQtpAqa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972149476,
     "user_tz": -120,
     "elapsed": 4572,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    }
   },
   "source": [
    "class TwoWay:\n",
    "    def __init__(self):\n",
    "        self.d = {}\n",
    "\n",
    "    def add(self, k, v):\n",
    "        self.d[k] = v\n",
    "        self.d[v] = k\n",
    "\n",
    "    def remove(self, k):\n",
    "        self.d.pop(self.d.pop(k))\n",
    "\n",
    "    def get(self, k):\n",
    "        return self.d[k]\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "\n",
    "# INITIALIZERS\n",
    "\n",
    "X = np.loadtxt(\"gdrive/My Drive/transformer/resources/X.csv\", dtype=str, delimiter=',', encoding='utf-8')\n",
    "y = np.loadtxt(\"gdrive/My Drive/transformer/resources/y.csv\", dtype=str, delimiter=',', encoding='utf-8')\n",
    "X_val = np.loadtxt(\"gdrive/My Drive/transformer/resources/X_val.csv\", dtype=str, delimiter=',', encoding='utf-8')\n",
    "y_val = np.loadtxt(\"gdrive/My Drive/transformer/resources/y_val.csv\", dtype=str, delimiter=',', encoding='utf-8')\n",
    "X_test = np.loadtxt(\"gdrive/My Drive/transformer/resources/X_test.csv\", dtype=str, delimiter=',', encoding='utf-8')\n",
    "\n",
    "# !!!!!!!!!!!!!!!\n",
    "# encode_dataset(np.hstack((X, X_val)), np.hstack((y, y_val)))\n",
    "\n",
    "with open('gdrive/My Drive/transformer/resources/encoded_X.pickle', 'rb') as f:\n",
    "    two_way_X = pickle.load(f)\n",
    "with open('gdrive/My Drive/transformer/resources/encoded_y.pickle', 'rb') as f:\n",
    "    two_way_y = pickle.load(f)\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "val_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "val_accuracy = tf.keras.metrics.Mean(name='val_accuracy')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_accuracy')\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=len(two_way_X.d),\n",
    "    target_vocab_size=len(two_way_y.d),\n",
    "    pe_input=500,\n",
    "    pe_target=500,\n",
    "    rate=dropout_rate)\n",
    "\n",
    "checkpoint_path = f\"gdrive/My Drive/transformer/checkpoints_{sliding_window_X}x_{sliding_window_y}y_{EPOCHS}ep/train\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')\n",
    "\n",
    "\n",
    "def get_encoder_emb(two_way_X):\n",
    "    weights = transformer.tokenizer.embedding.get_weights()[0]\n",
    "    vocab = two_way_X\n",
    "\n",
    "    out_v = io.open('gdrive/My Drive/transformer/training_data/vectors_enc.tsv', 'w', encoding='utf-8')\n",
    "    out_m = io.open('gdrive/My Drive/transformer/training_data/metadata_enc.tsv', 'w', encoding='utf-8')\n",
    "    for index in range(len(vocab.d) // 2):\n",
    "        if index == 0:\n",
    "            continue  # skip 0, it's padding.\n",
    "        vec = weights[index]\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "        out_m.write(vocab.get(index) + \"\\n\")\n",
    "    out_v.close()\n",
    "    out_m.close()\n",
    "\n",
    "def get_decoder_emb(two_way_y):\n",
    "    weights = transformer.decoder.embedding.get_weights()[0]\n",
    "    vocab = two_way_y\n",
    "\n",
    "    out_v = io.open('gdrive/My Drive/transformer/training_data/vectors_dec.tsv', 'w', encoding='utf-8')\n",
    "    out_m = io.open('gdrive/My Drive/transformer/training_data/metadata_dec.tsv', 'w', encoding='utf-8')\n",
    "    for index in range(len(vocab.d) // 2):\n",
    "        if index == 0:\n",
    "            continue  # skip 0, it's padding.\n",
    "        vec = weights[index]\n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "        out_m.write(vocab.get(index) + \"\\n\")\n",
    "    out_v.close()\n",
    "    out_m.close()\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def fit(train_batches, val_batches):\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        val_loss.reset_states()\n",
    "        val_accuracy.reset_states()\n",
    "\n",
    "        for (batch, (inp, tar)) in enumerate(train_batches):\n",
    "            train_step(inp, tar)\n",
    "            inp_val, tar_val = val_batches[batch % len(val_batches)]\n",
    "            validation_step(inp_val, tar_val)\n",
    "\n",
    "            if batch % PRINT_EACH_BATCHES == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch + 1} Batch {batch}'\n",
    "                    f' - Loss {train_loss.result():.4f}'\n",
    "                    f' - Accuracy {train_accuracy.result():.4f}'\n",
    "                    f' - Val Loss {val_loss.result():.4f}'\n",
    "                    f' - Val Accuracy {val_accuracy.result():.4f}'\n",
    "                )\n",
    "\n",
    "                train_accuracies.append(train_accuracy.result())\n",
    "                val_accuracies.append(val_accuracy.result())\n",
    "                train_losses.append(train_loss.result())\n",
    "                val_losses.append(val_loss.result())\n",
    "\n",
    "                if (epoch + 1) % SAVING_STEP == 0:\n",
    "                    ckpt_save_path = ckpt_manager.save()\n",
    "                    print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n",
    "                    np.save('gdrive/My Drive/transformer/training_data/train_accuracies.npy', train_accuracies)\n",
    "                    np.save('gdrive/My Drive/transformer/training_data/val_accuracies.npy', val_accuracies)\n",
    "                    np.save('gdrive/My Drive/transformer/training_data/train_losses.npy', train_losses)\n",
    "                    np.save('gdrive/My Drive/transformer/training_data/val_losses.npy', val_losses)\n",
    "\n",
    "                    get_encoder_emb(two_way_X)\n",
    "                    get_decoder_emb(two_way_y)\n",
    "\n",
    "        print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "        print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
    "\n",
    "    return np.array(train_accuracies), np.array(train_losses), np.array(val_accuracies), np.array(val_losses)\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp,\n",
    "                                     True,\n",
    "                                     enc_padding_mask,\n",
    "                                     combined_mask,\n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))\n",
    "\n",
    "\n",
    "@tf.function(input_signature=val_step_signature)\n",
    "def validation_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    predictions, _ = transformer(inp, tar_inp,\n",
    "                                 False,\n",
    "                                 enc_padding_mask,\n",
    "                                 combined_mask,\n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "    val_loss(loss)\n",
    "    val_accuracy(accuracy_function(tar_real, predictions))"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5uYNRK2sbmW"
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6ynLiX5NrNI9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1619972168591,
     "user_tz": -120,
     "elapsed": 23682,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    }
   },
   "source": [
    "# print((detokenize_(two_way_X, tokenize_pairs(X, y)[0][0])))\n",
    "# print((detokenize_(two_way_y, tokenize_pairs(X, y)[1][0])))\n",
    "\n",
    "\n",
    "train_batches = make_batches(tokenize_pairs(X, y), batch_size=BATCH_SIZE)\n",
    "val_batches = make_batches(tokenize_pairs(X_val, y_val), batch_size=BATCH_SIZE)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    },
    "id": "uTN7sagkRqNz",
    "executionInfo": {
     "status": "error",
     "timestamp": 1619972601863,
     "user_tz": -120,
     "elapsed": 456949,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    },
    "outputId": "2ef37ee7-ddff-487b-8ad2-ca29ffb3f999"
   },
   "source": [
    "#Saves pkl with new words not in X\n",
    "'''\n",
    "with open('gdrive/My Drive/transformer/resources/encoded_X.pickle', 'wb') as handle:\n",
    "    pickle.dump(two_way_X, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('gdrive/My Drive/transformer/resources/encoded_y.pickle', 'wb') as handle:\n",
    "    pickle.dump(two_way_y, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "'''\n",
    "\n",
    "# UNCOMMENT TO TRAIN\n",
    "# train_accuracies, train_losses, val_accuracies, val_losses = fit(train_batches, val_batches)\n",
    "\n",
    "def generator(length):\n",
    "    array_input = list(X_val[0:sliding_window_X])\n",
    "    last_input_array = []\n",
    "    for i in range(length):\n",
    "        if last_input_array:\n",
    "            [array_input.pop(0) for _ in range(sliding_window_y)]\n",
    "            for e in last_input_array:\n",
    "                array_input.append(re.sub(r'<syl>', '', e))\n",
    "        # from array to text\n",
    "        text_input = '<t_init>'\n",
    "        for k, s in enumerate(array_input):\n",
    "            if (k % 3 == 0) and (k > 0):\n",
    "                text_input += '<t_end>'\n",
    "                text_input += '<t_init>'\n",
    "            text_input += s\n",
    "        text_input += '<t_end>'\n",
    "        # end textification\n",
    "        text_input = re.sub(r'<c>', r'', text_input)\n",
    "        last_input = beam_search(text_input, two_way_X, two_way_y, max_length=5000)\n",
    "        last_input = re.sub(r'<t_init>|<t_end>', r'', last_input)\n",
    "        last_input = re.sub(r'<end>', r'<end>+', last_input)\n",
    "        last_input_array = last_input.split('+')\n",
    "        for l in last_input_array:\n",
    "            print(make_human_understandable(l, True))\n",
    "        print()\n",
    "\n",
    "\n",
    "generator(10)"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [1,501,256] vs. [1,500,256] [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-24-113293860068>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m \u001B[0mgenerator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-24-113293860068>\u001B[0m in \u001B[0;36mgenerator\u001B[0;34m(length)\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[0;31m# end textification\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[0mtext_input\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msub\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr'<c>'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mr''\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext_input\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m         \u001B[0mlast_input\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbeam_search\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtwo_way_X\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtwo_way_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m         \u001B[0mlast_input\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msub\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr'<t_init>|<t_end>'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mr''\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_input\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[0mlast_input\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msub\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr'<end>'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mr'<end>+'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlast_input\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-16-00a62938e485>\u001B[0m in \u001B[0;36mbeam_search\u001B[0;34m(sentence, two_way_X, two_way_y, max_length)\u001B[0m\n\u001B[1;32m     66\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m                 new_out = tf.concat(\n\u001B[0;32m---> 68\u001B[0;31m                     [(copy.deepcopy(e)).output, next_predictions(encoder_input, e.output, k=K, argmax=True)], axis=-1)\n\u001B[0m\u001B[1;32m     69\u001B[0m                 \u001B[0mnew_flow_array\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mFlow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_out\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprob\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-16-00a62938e485>\u001B[0m in \u001B[0;36mnext_predictions\u001B[0;34m(encoder_input, output, k, argmax)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mnext_predictions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mencoder_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margmax\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0menc_padding_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcombined_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdec_padding_mask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_masks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mencoder_input\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m     predictions, attention_weights = transformer(encoder_input,\n\u001B[0m\u001B[1;32m     29\u001B[0m                                                  \u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m                                                  \u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/TryingCuda/venv38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1010\u001B[0m         with autocast_variable.enable_auto_cast_variables(\n\u001B[1;32m   1011\u001B[0m             self._compute_dtype_object):\n\u001B[0;32m-> 1012\u001B[0;31m           \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1013\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1014\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-19-7c8ed62276b5>\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask)\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0menc_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menc_padding_mask\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (batch_size, inp_seq_len, d_model)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0;31m# dec_output.shape == (batch_size, tar_seq_len, d_model)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m         dec_output, attention_weights = self.decoder(\n\u001B[0m\u001B[1;32m     15\u001B[0m             tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\u001B[1;32m     16\u001B[0m         \u001B[0mfinal_output\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfinal_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdec_output\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (batch_size, tar_seq_len, target_vocab_size)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/TryingCuda/venv38/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1010\u001B[0m         with autocast_variable.enable_auto_cast_variables(\n\u001B[1;32m   1011\u001B[0m             self._compute_dtype_object):\n\u001B[0;32m-> 1012\u001B[0;31m           \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1013\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1014\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-17-9d57b28b0514>\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, x, enc_output, training, look_ahead_mask, padding_mask)\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# (batch_size, target_seq_len, d_model)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m*=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0md_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 91\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpos_encoding\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0mseq_len\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     92\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtraining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_layers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/TryingCuda/venv38/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mbinary_op_wrapper\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m   1162\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mop_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1163\u001B[0m       \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1164\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1165\u001B[0m       \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1166\u001B[0m         \u001B[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/TryingCuda/venv38/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    199\u001B[0m     \u001B[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    200\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 201\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    202\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    203\u001B[0m       \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/TryingCuda/venv38/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36m_add_dispatch\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m   1484\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1485\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1486\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_v2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1487\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1488\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/TryingCuda/venv38/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36madd_v2\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m    470\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    471\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 472\u001B[0;31m       \u001B[0m_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    473\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m       \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/TryingCuda/venv38/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001B[0m in \u001B[0;36mraise_from_not_ok_status\u001B[0;34m(e, name)\u001B[0m\n\u001B[1;32m   6860\u001B[0m   \u001B[0mmessage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m\" name: \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6861\u001B[0m   \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6862\u001B[0;31m   \u001B[0msix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6863\u001B[0m   \u001B[0;31m# pylint: enable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6864\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/TryingCuda/venv38/lib/python3.8/site-packages/six.py\u001B[0m in \u001B[0;36mraise_from\u001B[0;34m(value, from_value)\u001B[0m\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Incompatible shapes: [1,501,256] vs. [1,500,256] [Op:AddV2]"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sx1SxSpCynEj",
    "pycharm": {
     "name": "#%%\n"
    },
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1619972601860,
     "user_tz": -120,
     "elapsed": 456938,
     "user": {
      "displayName": "Serban Cristian Tudosie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKcHtWlRmEx5gPgHp_XElg5R9_lOdYkNWR-WiUiw=s64",
      "userId": "15345692188992114213"
     }
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
    "    translated_tokens = translated_tokens[1:]\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(attention)\n",
    "    ax.set_xticks(range(len(in_tokens)))\n",
    "    ax.set_yticks(range(len(translated_tokens)))\n",
    "    labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "\n",
    "def print_acc_loss():\n",
    "    train_accuracies = np.load(f'gdrive/My Drive/transformer/training_data_{sliding_window_X}x_{sliding_window_y}y_{EPOCHS}ep/train_accuracies.npy')\n",
    "    val_accuracies = np.load(f'gdrive/My Drive/transformer/training_data_{sliding_window_X}x_{sliding_window_y}y_{EPOCHS}ep/val_accuracies.npy')\n",
    "    train_losses = np.load(f'gdrive/My Drive/transformer/training_data_{sliding_window_X}x_{sliding_window_y}y_{EPOCHS}ep/train_losses.npy')\n",
    "    val_losses = np.load(f'gdrive/My Drive/transformer/training_data_{sliding_window_X}x_{sliding_window_y}y_{EPOCHS}ep/val_losses.npy')\n",
    "\n",
    "    x = np.linspace(1, EPOCHS, len(train_accuracies))\n",
    "\n",
    "    plt.title(\"Accuracies\")\n",
    "    plt.plot(x, train_accuracies)\n",
    "    plt.plot(x, val_accuracies)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend([\"train_acc\", \"val_acc\"])\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'gdrive/My Drive/transformer/training_data_{sliding_window_X}x_{sliding_window_y}y_{EPOCHS}ep'\n",
    "                f'/Acc_{sliding_window_X}x_{sliding_window_y}y_{EPOCHS}ep_chars.png')\n",
    "    plt.show()\n",
    "    print()\n",
    "\n",
    "    plt.title(\"Losses\")\n",
    "    plt.plot(x, train_losses)\n",
    "    plt.plot(x, val_losses)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend([\"train_loss\", \"val_loss\"])\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'gdrive/My Drive/transformer/training_data_{sliding_window_X}x_{sliding_window_y}y_{EPOCHS}ep'\n",
    "                f'/Loss_{sliding_window_X}x_{sliding_window_y}y_{EPOCHS}ep_chars.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print_acc_loss()"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}